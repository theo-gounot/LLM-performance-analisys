These codes were made in order to help me analise the performance of various open-source LLM models.
Metrics such as Tokens/second and the CPU/GPU stats while running LLM models. This was moved to a prometheus-grafana local infraestructure, useing llama.cpp-server.
